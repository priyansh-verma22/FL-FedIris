{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3c632f3-cc7e-42f5-b27a-a4c417b1e658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction complete.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Step 2: Define paths\n",
    "zip_path = r'C:\\Users\\priya\\Desktop/Iris_Recognition/Gpt/IITD_database-20250625T182513Z-1-001.zip'\n",
    "extract_path = r'C:\\Users\\priya\\Desktop\\Gpt\\Extracted'  # Destination after unzipping\n",
    "\n",
    "# Step 3: Unzip the file\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_path)\n",
    "\n",
    "print(\"Extraction complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "14bb9f33-0df9-444e-940e-a0f0bc9d6ced",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bd362cb-e7dc-428d-9b44-7be103a10d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\users\\priya\\.conda\\envs\\iris_reco\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\priya\\.conda\\envs\\iris_reco\\lib\\site-packages (from tqdm) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "! pip install tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "979ce4c6-01ea-46d7-992e-057d6d1664ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Client 1 with 74 classes...\n",
      "Processing Client 2 with 74 classes...\n",
      "Processing Client 3 with 74 classes...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "def get_all_images_recursive(class_path):\n",
    "    images = []\n",
    "    for root, _, files in os.walk(class_path):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp')):\n",
    "                images.append(os.path.join(root, file))\n",
    "    return images\n",
    "\n",
    "def split_dataset_clientwise(input_dir, output_base_dir, num_clients=3, classes_per_client=74, train_ratio=0.625, val_ratio=0.25, test_ratio=0.125):\n",
    "    random.seed(42)\n",
    "\n",
    "    # Get all valid class folders\n",
    "    class_folders = sorted([\n",
    "        folder for folder in os.listdir(input_dir)\n",
    "        if os.path.isdir(os.path.join(input_dir, folder))\n",
    "    ])\n",
    "\n",
    "    total_required_classes = classes_per_client * num_clients\n",
    "    if len(class_folders) < total_required_classes:\n",
    "        raise ValueError(f\"Not enough classes in the dataset. Required: {total_required_classes}, Found: {len(class_folders)}\")\n",
    "\n",
    "    selected_classes = class_folders[:total_required_classes]\n",
    "\n",
    "    # Shuffle for randomness\n",
    "    random.shuffle(selected_classes)\n",
    "\n",
    "    client_class_map = {}\n",
    "    for i in range(num_clients):\n",
    "        start = i * classes_per_client\n",
    "        end = start + classes_per_client\n",
    "        client_class_map[i + 1] = selected_classes[start:end]\n",
    "\n",
    "    for client_id, class_list in client_class_map.items():\n",
    "        print(f\"Processing Client {client_id} with {len(class_list)} classes...\")\n",
    "        output_dir = os.path.join(output_base_dir, f\"client{client_id}\")\n",
    "        if os.path.exists(output_dir):\n",
    "            shutil.rmtree(output_dir)\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        for class_name in class_list:\n",
    "            class_path = os.path.join(input_dir, class_name)\n",
    "            images = get_all_images_recursive(class_path)\n",
    "            if not images:\n",
    "                print(f\"Skipping empty class folder: {class_path}\")\n",
    "                continue\n",
    "            random.shuffle(images)\n",
    "\n",
    "            total = len(images)\n",
    "            train_end = int(train_ratio * total)\n",
    "            val_end = train_end + int(val_ratio * total)\n",
    "\n",
    "            train_imgs = images[:train_end]\n",
    "            val_imgs = images[train_end:val_end]\n",
    "            test_imgs = images[val_end:]\n",
    "\n",
    "            for split_name, split_imgs in zip(['train', 'val', 'test'], [train_imgs, val_imgs, test_imgs]):\n",
    "                split_dir = os.path.join(output_dir, split_name, class_name)\n",
    "                os.makedirs(split_dir, exist_ok=True)\n",
    "                for img_path in split_imgs:\n",
    "                    img_name = os.path.basename(img_path)\n",
    "                    dst = os.path.join(split_dir, img_name)\n",
    "                    shutil.copy2(img_path, dst)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_dataset_folder = r'C:\\Users\\priya\\Desktop\\Gpt\\Extracted\\IITD_database'\n",
    "    output_base_folder = r'C:\\Users\\priya\\Desktop\\Iris_Recognition\\Final_Dataset2\\Clients'\n",
    "\n",
    "    split_dataset_clientwise(input_dataset_folder, output_base_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb7607be-e256-4bd2-b9ba-f34f18b35282",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from glob import glob\n",
    "\n",
    "def merge_test_datasets(input_dirs, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    for input_dir in input_dirs:\n",
    "        for class_name in os.listdir(input_dir):\n",
    "            class_input_path = os.path.join(input_dir, class_name)\n",
    "            class_output_path = os.path.join(output_dir, class_name)\n",
    "\n",
    "            if not os.path.isdir(class_input_path):\n",
    "                continue\n",
    "\n",
    "            os.makedirs(class_output_path, exist_ok=True)\n",
    "\n",
    "            for file_path in glob(os.path.join(class_input_path, \"*.*\")):\n",
    "                # Create a unique filename to avoid name collisions\n",
    "                file_name = os.path.basename(file_path)\n",
    "                new_file_name = f\"{os.path.basename(input_dir)}_{file_name}\"\n",
    "                dest_path = os.path.join(class_output_path, new_file_name)\n",
    "                shutil.copy(file_path, dest_path)\n",
    "\n",
    "    print(f\"âœ… Merged datasets saved to: {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208f3b45-ffb0-41b1-9232-307503b06f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "merge_test_datasets(\n",
    "    input_dirs=[\n",
    "        r\"\",\n",
    "        r\"\",\n",
    "        r\"\"\n",
    "    ],\n",
    "    output_dir=r\"C:\\Users\\priya\\Desktop\\Iris_Recognition\\test_combined\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
